1. 正则化的作用及原理（L1/L2)正则化
   Lasso Regression Ridge Regression
   L1正则化产生稀疏的权值，L2正则化产生平滑的权值。L1能产生等于0的权值，**即能剔除某些特征在模型中的作用（特征选择）**，即产生稀疏的效果。L2可以迅速得到比较小的权值，但是难以收敛到0，因此产生的是**平滑的效果**。L2计算方便，同时L1在非稀疏向量上的计算效率很低。L2正则项可使参数在零附近稠密而平滑。
2. 衡量机器学习模型有哪些指标
   准确率、精度(查准率)、召回率(查全率)，误报率，ts，混淆矩阵
3. GBDT中的残差
4. 特征工程、特征选择、数据降维
5. 模型过拟合、欠拟合
   欠拟合是模型不能在训练集上获得足够低的误差。模型的复杂度低，模型在训练集上表现很差，没法学习到数据背后的规律。增加网络复杂度或者在模型中增加特征
   过拟合是训练误差和测试误差之间的差距过大，模型在训练集上表现很好，在测试集上表现很差，没有理解数据背后的规律，泛化能力差。
   训练数据集样本单一，样本不足
   训练数据中噪声干扰过大
   模型过于复杂
   L1、L2正则化参数分布
   Dropout，Early stopping

6. 先验分布、后验分布
7. 模型待调参数、模型超参数、模型如何调参
8. Python中有哪些数据结构，各有什么区别，适用于处理什么问题
   列表、字典、元组、字符串、集合、队列
9.  推荐算法
10. 聚类算法
11. kmeans原理与步骤
12. 数据类别不均衡怎么处理
    欠采样ENN、过采样SMOTE
13. CNN与RNN的区别
    CNN对于输入数据的维度约束比较严重，RNN更多地考虑了神经元间的联系(看图说话，语义情感分析、机器翻译)
    DNN存在局限：参数数量膨胀，局部最优，梯度消失，无法对时间序列上的变化进行建模
    RNN最重要的创新是提出把上一时刻的状态和当前时刻的输入联系在一起，计算当前输出。
    RNN能处理任意长度的输入，RNN的模型参数是共享的。
14. RNN为什么难以训练，LSTM又做了什么改进
   LSTM核心结构在于长期记忆，既能影响每个时刻的输出，又可以根据上一时刻的状态和当前时刻的输入进行调整，
15. SVM的核函数：线性核函数linear、多项式核函数pkf、高斯核函数rbf
    当训练数据线性可分，用linear；
    当训练数据不可分，使用核技巧，将训练数据映射到另一个高维空间，使在高维空间中数据可线性划分。如果特征数量小，样本数量正常，选用rbf。
16. SVM和LR的区别
    - 相同点：都是分类算法；不考虑核函数，都是线性分类算法，分类决策面都是线性的；监督学习算法；都是判别模型，区别于生成模型；
    - 不同：loss function不同，LR基于概率理论，SVM基于几何间隔最大化原理；SVM只考虑局部的边界线附近的点，LR考虑全局，依赖于数据分布；SVM采用核函数机制解决非线性问题；线性SVM依赖数据表达的距离测度，需要先对数据做normalization，LR不受其影响，一个基于概率，一个基于距离；SVM损失函数自带正则。
17. 决策树避免过拟合
    产生过拟合原因：
    * 样本问题：样本里的噪声数据干扰过大；样本抽取错误；建模时使用了样本中过多无关的输入变量。
    * 构建决策树的方法：对于决策树的生长没有合理的限制和剪枝
    剪枝：
    先剪枝(prepruning),树高，信息增益阈值、实例阈值。
    后剪枝(postpruning),REP方法可用数据分为两个样例集合，一部分构建树，一部分剪枝；PEP，根据剪枝前后的错误率来判定子树的修剪。
18. 决策树原理  
    ID3、C4.5 CART
    - 适用问题范围不同
        ID3处理离散特征的分类，C4.5处理离散特征和连续特征的分类问题，CART处理离散和连续特征的分类与回归
    - 假设空间不同
        ID3和C4.5决策树可以多分叉，CART决策树必须是二叉树
    - 目标函数不同
        决定哪个特征进行决策树的分裂时，ID3以信息增益为标准，C4.5以信息增益率为标准，CART以基尼不纯度增益为标准
    - 优化算法不同
        ID3算法实际上没有剪枝策略，当叶子节点上的样本都属于同一个类别或者所有特征都使用过了的情况下决策树停止生长
        C4.5算法使用预剪枝策略，当分裂后的增益小于给定阈值或者叶子上的样本数量小于某个阈值或者叶子节点数量达到限定值或者树的深度达到限定值，决策树停止生长。
    - 算法出现先后及效果不同
19. 随机森林和决策树的区别  
    决策树是一种非参数的监督学习方法，主要应用于分类和回归问题。决策树模型通过一系列if then决策规则的集合，将特征空间划分成有限个不相交的子区域，对于落在相同子区域的样本，决策树模型给出相同的预测值。
    随机森林是有一个包含多个决策树的分类器，并且其输出类别是由个别树输出的类别的众数而定。
20. 随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数所决定的。随机森林的随机性体现在每棵树的训练样本是随机的，树中每个节点的分裂属性集合也是随机选择确定的。有了这两个随机的保证，随机森林就不会产生过拟合的现象了。  
优点：
    - 能够处理高维度数据，并且不用做特征选择；
    - 创建随机森林时，对generalization error使用的是无偏估计，模型泛化能力强；
    - 训练速度快；易并行；
    - 训练过程中能够检测到feature间的影响；训练完后能够给出feature的重要性；
    - 对不平衡的数据集，可以平衡误差；如果有很大一部分的特征遗失，仍可维持准确度。
缺点：
    - 随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合；
    - 对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。
22. CART树，分类问题：每棵树投票找最高票；  
    回归问题：每棵树的值求和平均  
    RF 基于Bagging，每次训练随机从总数据D中选择N条数据，N<D。每次选择的特征是从总特征树P中，随机选择Q个特征，通常Q<P。重复M次，生成M棵树。  
    随机森林采用bagging思想，GBDT采用boosting思想；后者只能由回归树组成、串行生成。RF采取多数投票，GBDT将所有结果累加起来，或者加权累加起来。后者对异常值较敏感。RF是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。  
    方差，描述预测值的变化范围，离散程度，即离其期望值的距离。方差越大，数据的分布越分散。  
    偏差，描述预测值的期望与真实值之间的差距，偏差越大，越偏离真实数据。  
    XGboost中，增加树的数量，增加层数，来减少模型的偏差；通过cross-validation,正则化，减少模型的方差。  
    相对于GBDT，XGboost对代价函数进行二阶泰勒展开，同时用到了一阶导和二节导。还增加了正则项，用于控制模型的复杂度。XGboost支持列抽样进行随机特征选择，降低过拟合，减少计算。GBDT计算信息增益的是gini，而XGboost是优化推导公式。XGboost在处理每个特征列时采用并行化处理。  
23. 决策树不可能不限制地生长，总有停止分裂的 时候，最极端的情况是当节点分裂到只剩下一个数据点时自动结束分裂，但这种情况下树过于复杂，而且预测的精度不高。一般情况下下为了降低决策树的复杂度和提高预测精度，会适当提前终止节点分裂。
    - 最小节点数
        当节点的数据量小于一个指定的数量时，不继续分裂。两个原因：一是数据量较少时，再做分裂容易强化噪声数据的作用；二是降低树生长的复杂性。提前结束分裂一定程度上有利于降低过拟合的影响。
    - 熵或者基尼值小于阈值
        熵和基尼值的大小表示数据的复杂程度，当熵或者基尼值过小时，表示数据的纯度比较大，如果熵或者基尼值小于一定程度数，节点停止分裂。
    - 决策树的深度达到指定的条件
        决策树的深度是所有叶子节点的最大深度，当深度达到指定的上限大小时，停止分裂。
    - 所有特征已经使用完毕，不能继续进行分裂。
24. 分类树使用信息增益或增益比率来划分节点， 每个节点样本的类别情况投票决定测试样本的类别。  
    回归树使用最大均方差划分节点；每个节点样本的均值作为测试样本的回归预测值。
25. XGBoost的优缺点  
与GBDT对比  
1.GBDT的基分类器只支持CART树，而XGBoost支持线性分类器，此时相当于带有L1和L2正则项的逻辑回归（分类问题）和线性回归（回归问题）。  
2.GBDT在优化时只使用了一阶倒数，而XGBoost对目标函数进行二阶泰勒展开，此外，XGBoost支持自定义损失函数，只要损失函数二阶可导  
3.XGBoost借鉴随机森林算法，支持列抽样和行抽样，这样即能降低过拟合风险，又能降低计算。  
4.XGBoost在目标函数中引入了正则项，正则项包括叶节点的个数及叶节点的输出值的L2范数。通过约束树结构，降低模型方差，防止过拟合。  
5.XGBoost对缺失值不敏感，能自动学习其分裂方向  
6.XGBoost在每一步中引入缩减因子，降低单颗树对结果的影响，让后续模型有更大的优化空间，进一步防止过拟合。  
7.XGBoost在训练之前，对数据预先进行排序并保存为block，后续迭代中重复使用，减少计算，同时在计算分割点时，可以并行计算  
8.可并行的近似直方图算法，树结点在进行分裂时，需要计算每个节点的增益，若数据量较大，对所有节点的特征进行排序，遍历的得到最优分割点，这种贪心法异常耗时，这时引进近似直方图算法，用于生成高效的分割点，即用分裂后的某种值减去分裂前的某种值，获得增益，为了限制树的增长，引入阈值，当增益大于阈值时，进行分裂；  
与LightGBM对比  
1.XGBoost采用预排序，在迭代之前，对结点的特征做预排序，遍历选择最优分割点，数据量大时，贪心法耗时，LightGBM方法采用histogram算法，占用的内存低，数据分割的复杂度更低，但是不能找到最精确的数据分割点。同时，不精确的分割点可以认为是降低过拟合的一种手段。  
2.LightGBM借鉴Adaboost的思想，对样本基于梯度采样，然后计算增益，降低了计算  
3.LightGBM对列进行合并，降低了计算  
4.XGBoost采样level-wise策略进行决策树的生成，同时分裂同一层的节点，采用多线程优化，不容易过拟合，但有些节点分裂增益非常小，没必要进行分割，这就带来了一些不必要的计算；LightGBM采样leaf-wise策略进行树的生成，每次都选择在当前叶子节点中增益最大的节点进行分裂，如此迭代，但是这样容易产生深度很深的树，产生过拟合，所以增加了最大深度的限制，来保证高效的同时防止过拟合  
  